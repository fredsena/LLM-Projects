{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b3a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U transformers accelerate ctransformers langchain torch pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43894002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Type, Optional\n",
    "import json\n",
    "from langchain.llms import CTransformers\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708e4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5620bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'mistral-7b-instruct-v0.2.Q4_K_M.gguf'  # Replace this with the path to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce01ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"context_length\": 4096,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.9,\n",
    "    \"stream\": True,\n",
    "    \"threads\": int(os.cpu_count() / 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00ac301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(model=MODEL_PATH,\n",
    "                    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01216560",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886feb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 06:13:39.950439: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-10 06:13:39.950478: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-10 06:13:39.961664: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-10 06:13:40.901540: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-10 06:14:05.037791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd4c0d241824eecb9c918484ba90ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50400accd28b4a7eb96084d1c5452095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6d3c2f36b64a44a57e8ea0506b62d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e5aac0a7e14396a2653d57a739fd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2f8c610a07491c9d508c196ecf0071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db0bc07f3d5453399521bc182a7086e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d4fb6be723462ca0cff1c53c099719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccd1e2d3-ded4-4262-8a90-637a9ae9a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel, Field\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from langchain.tools import tool\n",
    "import requests\n",
    "from PIL import Image\n",
    "from langchain.agents import Tool\n",
    "from langchain.tools import BaseTool\n",
    "\n",
    "class ImageCaptionerInput(BaseModel):\n",
    "    image_url: str = Field(..., description=\"URL of the image that is to be described\")     \n",
    "\n",
    "@tool(\"image_captioner\", return_direct=True, args_schema=ImageCaptionerInput)\n",
    "def image_captioner(image_url: str) -> str:\n",
    "    \"\"\"Provides information about the image\"\"\"\n",
    "    raw_image = Image.open(requests.get(image_url, stream=True).raw).convert('RGB')\n",
    "    inputs = blip_processor(raw_image, return_tensors=\"pt\")\n",
    "    out = blip_model.generate(**inputs)\n",
    "    return blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "tools = [image_captioner]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "206ce069-a8fa-4223-b058-4c44a64a3610",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'ModelMetaclass' and 'ModelMetaclass'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferWindowMemory\n\u001b[1;32m      7\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationBufferWindowMemory(\n\u001b[1;32m      8\u001b[0m     memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, output_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mOutputParser\u001b[39;00m(AgentOutputParser):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_format_instructions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m FORMAT_INSTRUCTIONS\n",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m, in \u001b[0;36mOutputParser\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_format_instructions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FORMAT_INSTRUCTIONS\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mAgentAction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mAgentFinish\u001b[49m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m# this will work IF the text is a valid JSON with action and action_input\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         response \u001b[38;5;241m=\u001b[39m parse_json_markdown(text)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'ModelMetaclass' and 'ModelMetaclass'"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentOutputParser\n",
    "from langchain.agents.conversational_chat.prompt import FORMAT_INSTRUCTIONS\n",
    "from langchain.output_parsers.json import parse_json_markdown\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"output\"\n",
    ")\n",
    "\n",
    "class OutputParser(AgentOutputParser):\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return FORMAT_INSTRUCTIONS\n",
    "\n",
    "    def parse(self, text: str) -> AgentAction | AgentFinish:\n",
    "        try:\n",
    "            # this will work IF the text is a valid JSON with action and action_input\n",
    "            response = parse_json_markdown(text)\n",
    "            action, action_input = response[\"action\"], response[\"action_input\"]\n",
    "            if action == \"Final Answer\":\n",
    "                # this means the agent is finished so we call AgentFinish\n",
    "                return AgentFinish({\"output\": action_input}, text)\n",
    "            else:\n",
    "                # otherwise the agent wants to use an action, so we call AgentAction\n",
    "                return AgentAction(action, action_input, text)\n",
    "        except Exception:\n",
    "            # sometimes the agent will return a string that is not a valid JSON\n",
    "            # often this happens when the agent is finished\n",
    "            # so we just return the text as the output\n",
    "            return AgentFinish({\"output\": text}, text)\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"conversational_chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd080be2-c80f-43f4-bc91-1359ff4af5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize output parser for agent\n",
    "parser = OutputParser()\n",
    "\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "# initialize agent\n",
    "agent = initialize_agent(\n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    early_stopping_method=\"generate\",\n",
    "    memory=memory,\n",
    "    agent_kwargs={\"output_parser\": parser}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa031f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_instruct = \"<s>[INST] \"\n",
    "end_instruct = \"[/INST]\"\n",
    "end_sentence = \"</s>\"\n",
    "system_message_plain = \"\"\"Assistant is a expert JSON builder designed to help user describe images.\n",
    "\n",
    "Assistant is able to respond to the User and use tools using JSON strings that contain \"action\" and \"action_input\" parameters.\n",
    "\n",
    "All of Assistant's communication is performed using this JSON format.\n",
    "\n",
    "Assistant can also use tools by responding to the user with tool use instructions in the same \"action\" and \"action_input\" JSON format. Tools available to Assistant are:\n",
    "\n",
    "- \"image_captioner\": Useful when you need to get information about the image\n",
    "  - To use the image_captioner tool, Assistant should write like so:\n",
    "    ```json\n",
    "    {{\"action\": \"image_captioner\",\n",
    "      \"action_input\": \"https://xyz.png\"}}\n",
    "    ```\n",
    "\n",
    "Here are some previous conversations between the Assistant and User:\n",
    "\"\"\"\n",
    "system_message = start_instruct + system_message_plain + end_instruct + end_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6185fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": 'Hey how are you today?'\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": '''```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"I'm good thanks, how are you?\"}}\n",
    "```'''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I'm great, what is this image about - https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg ?\"\n",
    "        },\n",
    "            {\"role\": \"assistant\",\n",
    "            \"content\": '''```json\n",
    "{{\"action\": \"image_captioner\",\n",
    " \"action_input\": \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\"}}\n",
    "```'''\n",
    "        },{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"a woman sitting on the beach with her dog\"\n",
    "        },\n",
    "            {\"role\": \"assistant\",\n",
    "            \"content\": '''```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"This image shows a woman sitting on the beach with her dog\"}}\n",
    "```'''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Thanks could you now tell me what is in this image: https://www.adorama.com/alc/wp-content/uploads/2015/05/stories-HRX5WXFyB64-unsplash.jpg\"\n",
    "        },\n",
    "            {\"role\": \"assistant\",\n",
    "            \"content\": '''```json\n",
    "{{\"action\": \"image_captioner\",\n",
    " \"action_input\": \"https://www.adorama.com/alc/wp-content/uploads/2015/05/stories-HRX5WXFyB64-unsplash.jpg\"}}\n",
    "```'''\n",
    "        },{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"a beach with sun setting in the background\"\n",
    "        },\n",
    "            {\"role\": \"assistant\",\n",
    "            \"content\": '''```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"The image is of a sunset on the beach\"}}\n",
    "```'''\n",
    "        }\n",
    "]\n",
    "\n",
    "conversation_formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e09b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = system_message + '\\n\\n' + conversation_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c22ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = agent.agent.create_prompt(\n",
    "    system_message=prompt,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b17aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.agent.llm_chain.prompt = new_prompt\n",
    "\n",
    "instruction = start_instruct + \" Respond to the following in JSON with 'action' and 'action_input' values \" + end_instruct\n",
    "human_msg = instruction + \"\\nUser: {input}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebebbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.agent.llm_chain.prompt.messages[2].prompt.template = human_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = agent(\"Explain this image: https://images.hindustantimes.com/auto/img/2023/07/23/1600x900/Tesla_Cybertruck_1688887534001_1690087911053.jpeg\")\n",
    "resp['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c843a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = agent('Where was the Tesla car parked?')\n",
    "resp['output']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
